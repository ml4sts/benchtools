# Design
"The tested capability, characteristic, or concept is defined": 
  - "Tested concept, capability, or characteristic not explicitly mentioned."
  - "Tested concept explicitly mentioned and need for definition acknowledged, but definition not provided."
  - "Tested concept, capability, or characteristic explicitly mentioned but not defined."
  - "Tested concept, capability, or characteristic explicitly mentioned and defined."

"How tested capability or concept translates to benchmark task is described":
  - "No description of how the tested capability or concept translates to the benchmark task."
  - "Acknowledgement that not describing how the tested capability or concept translates to the benchmark task is an issue, but no description provided."
  - "Description of how tested capability or concept translates to benchmark tasks provided for some but not all tasks."
  - "Description of how tested capability or concept translates to benchmark tasks provided for all tasks."

"How knowing about the tested concept is helpful in the real world is described":
  - "No description of how knowing about the tested concept is helpful in the real world."
  - "Acknowledgement that not describing how knowing about the tested concept is helpful in the real world is an issue, but no description provided."
  - "Limited description of how knowing about the tested concept is helpful in the real world."
  - "Full description of how knowing about the tested concept is helpful in the real world."

"How benchmark score should or shouldn't be interpreted/used is described":
  - "The benchmark does not comment on how the benchmark scores should or shouldn't be interpreted."
  - "The benchmark acknowledges that the benchmark scores need to be interpreted but gives no guidance with respect to how or how not to do that."
  - "The benchmark describes how scores should or shouldn't be interpreted or used, but not both."
  - "The benchmark describes how scores should and shouldn't be interpreted or used."

"Domain experts are involved":
  - "None of the authors has a background in the benchmark domain and no external experts were consulted during the design process."
  - "The benchmark mentions domain experts but doesn't specify any further details."
  - "The benchmark mentions that domain experts were consulted but not how their insights influenced the benchmark design."
  - "At least one of the co-authors has a professional or academic background in the benchmark domain or the benchmark specified how external experts were consulted and how that influenced the design process."

# has n/a
"Use cases and/or user personas are described":
  - "The benchmark does not include any description of use cases or user personas."
  - "The benchmark acknowledges the importance of use cases or user personas but does not explicitly formulate or describe them."
  - "The benchmark provides a partial description of use cases or user personas."
  - "The benchmark fully describes use cases and user personas, specifying the cultural and geographic context, types of human-model interactions (if applicable), and representing different user types that might interact with the AI system (if applicable)."
  - "For AI systems that do not involve direct human interaction, such as those used in industrial automation or scientific simulations, defining user personas is not relevant. However, real-world use cases should still be specified; in more theoretical benchmarks, this use case might be to advance research."

"Domain literature is integrated":
  - "The benchmark does not reference domain-specific literature."
  - "The benchmark mentions the need to integrate domain literature but did not address it in the background section or design process."
  - "The benchmark references domain literature in the background or related work section but does not describe how that domain literature informed the benchmark design process."
  - "The benchmark references domain literature throughout the paper and describes how that domain literature informed the benchmark design process."

"Informed performance metric choice":
  - "The benchmark does not mention an evaluation metric or does not explain the choice of metric."
  - "The benchmark acknowledges the need for an informed metric choice but does not justify their metric choice."
  - "The benchmark provides an explanation for the choice of some but not all of their metrics."
  - "The benchmark provides an explanation for the choice of all of their metrics."

"Metric floors and ceilings are included":
  - "The benchmark doesn't provide any floors or ceilings."
  - "Floors and ceilings are shown in the results figure but not explicitly mentioned in the text."
  - "The benchmark provides floors and ceilings for some but not all evaluation metrics."
  - "The benchmark provides floors and ceilings for all evaluation metrics."

# has n/a
"Human performance level is included":
  - "The benchmark does not state human performance and does not explain why this isn't applicable here."
  - "The benchmark mentions human performance in passing but does not provide a measurement or explanation."
  - "The benchmark states human performance but does not explain how it was obtained."
  - "The benchmark states human performance and explains how it was obtained."
  - "The benchmark task cannot be completed by a human, and hence reporting human performance is not possible."
 
# has n/a
"Random performance level is included":
  - "The benchmark does not state random performance and does not explain why this isn't applicable here."
  - "The benchmark mentions random performance but does not provide quantitative random performance on the benchmark task(s)."
  - "The benchmark states random performance for some but not all tasks."
  - "The benchmark states random performance for all tasks."
  - "Measuring random performance on the benchmark task is not possible, and hence reporting random performance is not possible."
 
"Automatic evaluation is possible and validated":
  - "The benchmark does not provide any form of automatic evaluation and relies entirely on human evaluation."
  - "The benchmark mentions the benefits of automatic evaluation but provides no none or limited automatic valuation."
  - "The benchmark includes an automatic evaluation method but does not offer any validation."
  - "The benchmark includes an automatic evaluation method and describes how it was validated as well as the results of the validation."

"Differences to related benchmarks are explained":
  - "The benchmarks do not explain any differences or relevance to existing benchmarks."
  - "The benchmark briefly mentions existing benchmarks but provides no explanations of differences or added value."
  - "The benchmark provides an explanation of how it fills a gap or expands on existing benchmarks for some but not all mentioned related benchmarks."
  - "The benchmark provides an explanation of how it fills a gap or expands on existing benchmarks for all mentioned related benchmarks."
 
"Input sensitivity is addressed":
  - "The benchmark does not mention or address input sensitivity."
  - "The benchmark mentions the issue of input sensitivity but does not describe experiments to test for it."
  - "The benchmark includes some input variations with the same semantic meaning but lacks thorough descriptions or details on the number of variations and their design."
  - "The benchmark contains multiple input variations with the same semantic meaning, providing detailed descriptions of all relevant details such as the number of variations per prompt and how they were designed."
 

# Implementation
"The evaluation code is available":
  - "The evaluation code is not publicly available."
  - "The benchmark mentions the availability of evaluation code but does not provide access to it."
  - "The evaluation code is publicly available for some metrics described by the benchmark."
  - "The evaluation code is publicly available for all metrics described by the benchmark."

"The evaluation data or generation mechanism is accessible":
  - "No access to evaluation data, prompts, or data/environment generation mechanism is provided."
  - "The existence of evaluation data, prompts, or data/environment generation mechanism is mentioned, but no concrete access is provided."
  - "Partial access to evaluation data, prompts, or data/environment generation mechanism is provided, allowing for limited evaluation."
  - "Full access to evaluation data, prompts, or data/environment generation mechanism is provided, enabling comprehensive evaluation."

"The evaluation of models via API is supported":
  - "The benchmark does not support evaluation of models via API calls."
  - "The benchmark mentions the possibility of API evaluation but does not provide concrete implementation details."
  - "The benchmark supports evaluation of models via one API."
  - "The benchmark supports evaluation of models via two or more APIs to different models."
 
"The evaluation of local models is supported":
  - "The benchmark requires users to write their own code to evaluate a local model."
  - "The benchmark mentions that local evaluation should be possible but doesn't provide corresponding code."
  - "The benchmark provides minimal support for local model evaluation, requiring significant user effort."
  - "The benchmark provides full support for local model evaluation with user-friendly code."

"A globally unique identifier is added or evaluation instances are encrypted":
  - "The benchmark does not include a GUID or encryption of evaluation instances."
  - "The benchmark acknowledges the risk of contamination but does not address it."
  - "The benchmark partially implements a GUID or encryption, but not consistently across all relevant files."
  - "The benchmark consistently includes a GUID or encryption across all relevant files and repositories."
 
"A task to identify if model is trained on benchmark data":
  - "The benchmark does not include a 'training_on_test_set' task."
  - "The benchmark mentions the possibility that models were trained on its data but does not provide a way to check it."
  - "The benchmark includes a partial or limited implementation of a 'training_on_test_set' task that only tests for part of the data used."
  - "The benchmark includes a comprehensive 'training_on_test_set' task."

"A script to replicate results is explicitly included":
  - "[No description provided]"
  - "The issue of result replicability is mentioned in the benchmark paper but not addressed."
  - "A script to reproduce some results in the benchmark paper is available."
  - "A script to reproduce all results in the benchmark paper is available."
 
"Statistical significance or uncertainty quantification of benchmark results is reported":
  - "No statistical significance testing or variance reporting is provided for the benchmark results."
  - "The need for valid benchmarks and/or statistical significance or uncertainty estimation is mentioned but not addressed."
  - "Benchmark developers bound the expected variation across model training runs."
  - "Benchmark developers run statistical significance tests on the benchmark results for at least one model and provide variance bounds or other uncertainty estimations. In cases where the benchmark is perfectly deterministic, this is explicitly stated."

"Need for warnings for sensitive/harmful content is assessed":
  - "The benchmark does not mention that they checked for the presence or absence of sensitive/harmful content in the evaluation tasks or expected output."
  - "The benchmark mentions the general possibility of sensitive/harmful content but does not provide clear statements or warnings."
  - "The benchmark explicitly states the presence or absence of sensitive/harmful content for either the evaluation tasks or the expected output."
  - "The benchmark explicitly states the presence or absence of sensitive/harmful content for both the evaluation tasks and the expected output."
 
"A build status (or equivalent) is implemented":
  - "The benchmark neither references nor implements any form of build status or equivalent."
  - "The benchmark mentions the need for working evaluation code but does not implement it in any meaningful way."
  - "The benchmark partially implements a build status or equivalent by providing the information in a less accessible manner."
  - "The benchmark fully implements a build status or equivalent, clearly displaying the status of the most recent build and providing easy access to the information."
 
"Release requirements are specified":
  - "The benchmark does not specify any release requirements for benchmark users."
  - "The benchmark briefly mentions the issue of potential gameability or misuse by benchmark users but does not provide specific details."
  - "The benchmark states dos and don'ts how to use the benchmark but does not specify these as requirements for use."
  - "The benchmark provides a set of release requirements for benchmark users."
 

# Documentation:
"Requirements file or equivalent is available":
  - "No requirements file or equivalent is provided."
  - "A requirements file is mentioned but not provided."
  - "A requirements file is provided but may be missing some dependencies or versions."
  - "A complete and accurate requirements file specifying all necessary dependencies and versions is provided."

"Quick-start guide or demo is available":
  - "No quick-start guide or demo code is provided."
  - "A quick-start guide or demo code is mentioned but not provided."
  - "A quick-start guide or demo code is provided but may be missing some steps or details."
  - "A comprehensive, step-by-step quick-start guide or demo code is provided."

"In-line code comments are used":
  - "No in-line code comments are provided."
  - "In-line code comments are sparse and do not adequately explain the purpose, inputs, outputs, or functionality of the code."
  - "Informative in-line code comments are present for most of the code but may be lacking in detail or clarity for some code segments."
  - "Comprehensive and informative in-line code comments are provided for all relevant code segments, clearly explaining their purpose, inputs, outputs, and functionality."

"Code documentation is available":
  - "No code documentation is provided."
  - "Code documentation is mentioned but not provided."
  - "Code documentation is minimal or incomplete, lacking important details about the repository structure and functions."
  - "Comprehensive code documentation is provided, including a clear overview of the folder structure, files in the repo, and detailed explanations of all relevant functions."

"Accompanying paper is accepted at peer-reviewed venue":
  - "The benchmark/its associated paper has not been accepted at a peer-reviewed venue."
  - "The benchmark/its associated paper has been submitted to a peer-reviewed venue but is still under review or awaiting acceptance."
  - "The benchmark/its associated paper has been accepted at a peer-reviewed workshop or symposium."
  - "The benchmark/its associated paper has been accepted at a peer-reviewed journal, conference, or similar high-profile venue."

"Benchmark construction process is documented":
  - "No documentation of the benchmark construction process is provided."
  - "The benchmark construction process is briefly mentioned but lacks sufficient detail about the decisions made, rationale, and trade-offs considered."
  - "The benchmark construction process is documented, including some decisions made and their rationale, but the description lacks depth or fails to address important aspects such as trade-offs or compromises."
  - "The benchmark construction process is comprehensively documented, providing a detailed account of the specific decisions made at each stage, the rationale behind them, and any trade-offs or compromises considered."

"Test tasks & rationale are documented":
  - "No documentation of test task categories or rationale is provided."
  - "Test task categories are mentioned but they are neither defined in detail and a rationale for their selection is missing or inadequate."
  - "Test task categories are defined, but the rationale for their selection is not provided."
  - "Test task categories are clearly defined, and a comprehensive rationale is provided, explaining their relevance to the benchmark's objectives, what they measure, and their importance for evaluating the targeted concept or capability."

"Assumptions of normative properties are documented":
  - "No documentation of normative assumptions is provided, even though the benchmark measures culturally-dependent properties."
  - "The potential influence and importance of cultural context on the benchmark is acknowledged but normative assumptions aren't stated."
  - "Normative assumptions are stated, but the explanation of how they are conceptualized and operationalized within the benchmark is incomplete or lacks clarity."
  - "Normative assumptions are explicitly and clearly stated, defining the cultural context and values that the benchmark adheres to, and explaining how the measured properties are conceptualized and operationalized within the benchmark."

"Limitations are documented":
  - "No documentation of the benchmark's limitations is provided."
  - "Limitations of AI evaluations more broadly are briefly mentioned but without any detail and not applied to the specific benchmark."
  - "Either limitations regarding the applicability and use of the benchmark or limitations of the benchmark design are discussed, but not both."
  - "Both limitations regarding the applicability and use of the benchmark and limitations of the benchmark design are comprehensively discussed."

"Data collection, test environment design, or prompt design process is documented":

  - "No documentation of the data collection or environment/prompt design process is provided."
  - "The data collection or environment/prompt design process is briefly mentioned but no information about the sources, selection criteria, preprocessing steps, or prompt validation is provided."
  - "The data collection or environment/prompt design process is documented, including some information about the sources, selection criteria, preprocessing steps, or prompt validation, but the description lacks depth or fails to address important aspects."
  - "The data collection or environment/prompt design process is comprehensively documented, providing a detailed account of the sources of the data, the criteria for selection, any preprocessing steps, and, if applicable, how the prompts were created, validated, and why they were used to elicit the desired responses."
 
"Evaluation metric is documented":
  - "No documentation of the evaluation metrics is provided."
  - "The evaluation metrics are mentioned but not clearly defined, and the exact formulas or processes used to calculate them are not provided."
  - "The evaluation metrics are defined, but the documentation lacks some important details, such as any parameters or thresholds employed."
  - "The evaluation metrics are clearly specified. The exact formulas or processes used to calculate these metrics, along with any parameters or thresholds employed, are comprehensively documented."

"Applicable license is specified":
  - "No license is specified for the benchmark."
  - "A license is mentioned but not clearly specified or linked to in the code repository or paper."
  - "A license is specified but lacks some important details about the conditions under which the benchmark can be used, modified, or distributed."
  - "The applicable license for the benchmark is clearly specified in the code repository or paper, providing comprehensive information about the conditions under which the benchmark can be used, modified, and distributed."

# Maintenance:
"Code usability was checked within the last year":
  - "No updates to the main files of the public code within the last year, and no explicit statement of a usability check in the README file."
  - "Updates to minor files in the repo were made (e.g., README file) but an explicit statement of a usability check in the README file is not reported."
  - "Updates to the main files of the public code were made within the last year, but the build status check failed and wasn't fixed."
  - "Updates to the main files of the public code within the last year, accompanied by a successful build status check, or an explicit statement of a usability check in the README file, including the date of the check was provided."

"Maintained feedback channel for users is available":
  - "No acknowledgment or response to GitHub issues that are older than three months."
  - "GitHub issues are mentioned as a way to provide feedback but there are GitHub issues that were not responded to and that are older than three months."
  - "All GitHub issues are acknowledged within three months, but not all are addressed or resolved or were closed because the issue/feature request won't be attended to."
  - "All GitHub issues are acknowledged and addressed within three months, or it is clearly stated if an issue cannot be fixed or if a feature request won't be fulfilled. Alternatively, there are no open issues."

"Contact person is listed":
  - "It is not disclosed who developed the benchmark."
  - "The benchmark developers are disclosed but no explicit contact details are provided."
  - "Contact details are provided but are incomplete or difficult to find, e.g., only as part of terms of service on a website."
  - "Contact details of the person responsible for the benchmark are easily accessible, such as a corresponding author in the associated paper, a contact person listed on GitHub or the website, or an available online feedback form."